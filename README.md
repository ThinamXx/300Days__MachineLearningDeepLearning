# **Journey of 300DaysOfData in Machine Learning and Deep Learning**

![MachineLearning](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/ML.jpg)

| Books and Resources |
| ----- |
| 1. [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html) |
| 2. **A Comprehensive Guide to Machine Learning** |
| 3. **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow** |
| 4. [**Speech and Language Processing**](https://web.stanford.edu/~jurafsky/slp3/) |
| 5. [**Machine Learning Crash Course**](https://developers.google.com/machine-learning/crash-course) |
| 6. [**Deep Learning with PyTorch**](https://www.manning.com/books/deep-learning-with-pytorch) |


| Research Papers |
| --------------- |
| 1. [**Practical Recommendations for Gradient based Training of Deep Architectures**](https://arxiv.org/pdf/1206.5533.pdf) |


| Projects and Notebooks |
| ---------------------- |
| 1. [**California Housing Prices**](https://github.com/ThinamXx/CaliforniaHousing__Prices.git) |


**Day1 of 300DaysOfData!**
- **Gradient Descent and Cross Validation**: Gradient Descent is an iterative approach to approximating the Parameters that minimize a Differentiable Loss Function. Cross Validation is a resampling procedure used to evaluate Machine Learning Models on a limited Data sample which has a parameter that splits the data into number of groups. On my Journey of Machine Learning and Deep Learning, Today I have read in brief about the fundamental Topics such as Calculus, Matrices, Matrix Calculus, Random Variables, Density Functions, Distributions, Independence, Maximum Likelihood Estimation and Conditional Probability. I have also read and Implemented about Gradient Descent and Cross Validation. I am starting this Journey from Scratch and I am following the Book:**Machine Learning From Scratch**. I have presented the Implementation of Gradient Descent and Cross Validation here in the Snapshots. I hope you will also spend some time reading the Topics from the Book mentioned above. I am excited about the days to come!!
- Book:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%201.PNG)

**Day2 of 300DaysOfData!**
- **Ordinary Linear Regression**: Linear Regression is a linear approach to modelling the relationships between a scalar response or dependent variable and one or more explanatory variables or independent variables. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Ordinary Linear Regression, Parameter Estimation, Minimizing Loss and Maximizing Likelihood along with the Construction and Implementation of the LR from the Book **Machine Learning From Scratch**. I have also started reading the Book **A Comprehensive Guide to Machine Learning** which focuses on Mathematics and Theory behind the Topics. I have read about Regression, Ordinary Least Squares, Vector Calculus, Orthogonal Projection, Ridge Regression, Feature Engineering, Fitting Ellipses, Polynomial Features, Hyperparameters and Validation, Errors and Cross Validation from this book. I have presented the Implementation of Linear Regression along with Visualizations using Python here in the Snapshots. I hope you will also spend some time reading the Topics and Books mentioned above. Excited about the days ahead!!
- Books:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - **A Comprehensive Guide to Machine Learning**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%202a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%202b.PNG)

**Day3 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Regularized Regression such as Ridge Regression and Lasso Regression, Bayesian Regression, GLMs, Poisson Regression along with Construction and Implementation of the same from the Book **Machine Learning From Scratch**. I have also read the Book **A Comprehensive Guide to Machine Learning** which focuses on Mathematics and Theory behind the Topics. I have read about Maximum Likelihood Estimation or MLE and Maximum a Posteriori or MAE for Regression, Probabilistic Model, Bias Variance Tradeoff, Metrics, Bias Variance Decomposition, Alternative Decomposition, Multivariate Gaussians, Estimating Gaussians from Data, Weighted Least Squares, Ridge Regression, and Generalized Least Squares from this Book. I have presented the Implementation of Ridge Regression, Lasso Regression along with Cross Validation, Bayesian Regression and Poisson Regression using Python here in the Snapshot. I hope you will also spend some time reading the Topics and Books mentioned above. Excited about the days ahead!!
- Books:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - **A Comprehensive Guide to Machine Learning**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%203.PNG)

**Day4 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Discriminative Classifiers such as Binary and Multiclass Logistic Regression, The Perceptron Algorithm, Parameter Estimation, Fishers Linear Discriminant and Fisher Criterion along with Construction and Implementation of the same from the Book **Machine Learning From Scratch**. I have also read the Book **A Comprehensive Guide to Machine Learning** which focuses on Mathematics and Theory behind the Topics. I have read about Kernels and Ridge Regression, Linear Algebra Derivation, Computational Analysis, Sparse Least Squares, Orthogonal Matching Pursuit, Total Least Squares, Low rank Formulation, Dimensionality Reduction, Principal Component Analysis, Projection, Changing Coordinates, Minimizing Reconstruction Errors and Probabilistic PCA from this Book. I have presented the Implementation of Binary and Multiclass Logistic Regression, The Perceptron Algorithm and Fishers Linear Discriminant using Python here in the Snapshot. I hope you will also spend some time reading the Topics and Books mentioned above. Excited about the days ahead!!
- Books:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - **A Comprehensive Guide to Machine Learning**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%204.PNG)

**Day5 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Generative Classifiers such as Linear Discriminative Analysis or LDA, Quadratic Discriminative Analysis or QDA, Naive Bayes, Parameter Estimation and Data Likelihood along with Construction and Implementation of the same from the Book **Machine Learning From Scratch**. I have also read the Book **A Comprehensive Guide to Machine Learning** which focuses on Mathematics and Theory behind the Topics. I have read about Generative and Discriminative Classification, Bayes Decision Rule, Least Squares Support Vector Machines, Feature Extension, Neural Network Extension, Binary and Multiclass Logistic Regression, Loss Function, Training, Multiclass Extension, Gaussian Discriminant Analysis, QDA and LDA Classification and Support Vector Machines from this Book. I have presented the Implementation of LDA, QDA and Naive Bayes along with Visualizations using Python here in the Snapshot. I hope you will also spend some time reading the Topics and Books mentioned above. Excited about the days ahead!!
- Books:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - **A Comprehensive Guide to Machine Learning**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%205.PNG)

**Day6 of 300DaysOfData!**
- **Decision Trees**: A Decision Tree is an interpretable machine learning for Regression and Classification. It is a flow chart like structure in which each internal node represents a Test on an attribute and each branch represents the outcome of the Test. On my Journey of Machine Learning and Deep Learning, Today I have read about Decision Trees such as Regression Trees and Classification Trees, Building Trees, Making Splits and Predictions, Hyperparameters, Pruning and Regularization along with Construction and Implementation of the same from the Book **Machine Learning From Scratch**. I have also read the Book **A Comprehensive Guide to Machine Learning** which focuses on Mathematics and Theory behind the Topics. I have read about Decision Tree Learning, Entropy and Information, Gini Impurity, Stopping Criteria, Random Forests, Boosting and AdaBoost, Gradient Boosting and KMeans Clustering from this Book. I have presented the Implementation of Regression Trees and Classification Trees using Python here in the Snapshot. I hope you will also spend some time reading the Topics and Books mentioned above. Excited about the days ahead!!
- Books:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - **A Comprehensive Guide to Machine Learning**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%206.PNG)

**Day7 of 300DaysOfData!**
- **Tree Ensemble Methods**: Ensemble Methods combine the outputs of multiple simple Models which is often called Learners in order to create the fine Model with low variance. Due to their high variance, a decision trees often fail to reach a level of precision comparable to other predictive algorithms and Ensemble Methods minimize the variance. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Tree Ensemble Methods such as Bagging for Decision Trees, Bootstrapping, Random Forests and Procedure, Boosting, AdaBoost for Binary Classification, Weighted Classification Trees, The Discrete AdaBoost Algorithm and AdaBoost for Regression along with Construction and Implementation of the same from the Book **Machine Learning From Scratch**. I have presented the Implementation of Bagging, Random Forests and AdaBoost along with different base estimators using Python here in the Snapshot. I hope you will also spend some time reading the Topics and Book mentioned above. Excited about the days ahead !!
- Books:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%207.PNG)

**Day8 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Neural Networks from the Book **Machine Learning From Scratch**. I have read about Model Structure, Communication between Layers, Activation Functions such as ReLU, Sigmoid, The Linear Activation Function, Optimization, Back Propagation, Calculating Gradients, Chain Rule and Observations, Loss Functions along with Construction using The Loop Approach and The Matrix Approach and Implementation of the same from this Book. I have also read the Book **A Comprehensive Guide to Machine Learning** which focuses on Mathematics and Theory behind the Topics. I have read about Convolutional Neural Networks and Layers, Pooling Layers, Back Propagation for CNN, ResNet and Visual Understanding of CNNs from this Book. Besides, I have seen a couple of videos of Neural Networks and Deep Learning. I have presented the simple Implementation of Neural Networks with The Functional API and The Sequential API using TensorFlow here in the Snapshot. I hope you will also spend some time reading the Topics and Books mentioned above. Excited about the days ahead !!
- Books:
  - [**Machine Learning From Scratch**](https://dafriedman97.github.io/mlbook/content/introduction.html)
  - **A Comprehensive Guide to Machine Learning**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%208.PNG)

**Day9 of 300DaysOfData!**
- **Reinforcement Learning**: In Reinforcement Learning, The Learning system called an agent in a particular context can observe the environment, select and perform actions and get rewards in return or penalties in the form of negative rewards. It must learn by itself what is the best policy to get the most reward over time. On my Journey of Machine Learning and Deep Learning, Today I have started reading and Implementing from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have read briefly about The Machine Learning Landscape viz. Types of Machine Learning Systems such as Supervised and Unsupervised Learning, Semisupervised Learning, Reinforcement Learning, Batch Learning and Online Learning, Instance Based Learning and Model Based Learning from this Book. I have presented the simple Implementation of Linear Regression and KNearest Neighbors along with a simple plot using Python here in the Snapshot. I hope you will also spend some time reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%209a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%209b.PNG)

**Day10 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read about the Main Challenges of Machine Learning such as Insufficient Quantity of Training Data, Non representative Training Data, Poor Quality Data, Irrelevant Features, Overfitting and Underfitting the Training Data and Testing and Validating, Hyperparameter Tuning and Model Selection and Data Mismatch from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have started working on **California Housing Prices** Dataset which is included in this Book. I will build a Model of Housing Prices in California in this Project. I have presented the simple Implementation of Data Processing and few techniques of EDA using Python here in the Snapshot. I have also presented the Implementation of Sweetviz Library for Analysis here. I really appreciate Chanin Nantasenamat for sharing about this Library in one of his videos. I hope you will also spend some time reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
- [**Chanin Nantasenamat Video on Sweetviz**](https://www.youtube.com/watch?v=UR_OK8vBpeY&lc=z22itptbrzv0vfky504t1aokgq4l23pa5kermfzdyrfkbk0h00410.1605764911555430)
- [**California Housing Prices**](https://github.com/ThinamXx/CaliforniaHousing__Prices.git)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2010.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2010b.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2010a.PNG)

**Day11 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have learned and Implemented about Creating categories from attributes, Stratified Sampling, Visualizing Data to gain insights, Scatter Plots, Correlations, Scatter Matrix and Attribute Combinations from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have continued working with **California Housing Prices** Dataset which is included in this Book. This Dataset was based on Data from the 1990 California Census. I will build a Model of Housing Prices in California in this Project. I am still working on the same. I have presented the Implementation of Stratified Sampling, Correlations using Scatter Matrix and Attribute combinations using Python here in the Snapshots. I have also presented the Snapshots of Correlations using Scatter plots here. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead !! 
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
- [**California Housing Prices**](https://github.com/ThinamXx/CaliforniaHousing__Prices.git)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2011a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2011b.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2011c.PNG)

**Day12 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have learned and Implemented about Preparing the Data for Machine Learning Algorithms, Data Cleaning, Simple Imputer, Ordinal Encoder, OneHot Encoder, Feature Scaling, Transformation Pipeline, Standard Scaler, Column Transformer, Linear Regression, Decision Tree Regressor and Cross Validation from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have continued working with **California Housing Prices** Dataset which is included in this Book. This Dataset was based on Data from the 1990 California Census. I will build a Model of Housing Prices in California in this Project. The Notebook contains almost every Topics mentioned above. I have presented the Implementation of Data Preparation, Handling missing values, OneHot Encoder, Column Transformer, Linear Regression, Decision Tree Regressor along with Cross Validation using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead !!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
- [**California Housing Prices**](https://github.com/ThinamXx/CaliforniaHousing__Prices.git)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2012a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2012b.PNG)

**Day13 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have learned and Implemented about Random Forest Regressor, Ensemble Learning, Tuning the Model, Grid Search, Randomized Search, Analyzing the Best Models and Errors, Model Evaluation, Cross Validation and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have completed working with **California Housing Prices** Dataset which is included in this Book. This Dataset was based on Data from the 1990 California Census. I have built a Model using Random Forest Regressor of California Housing Prices Dataset to predict the price of the Houses in California. I have presented the Implementation of Random Forest Regressor and Tuning the Model with Grid Search and Randomized Search along with Cross Validation using Python here in the Snapshot. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!! 
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
- [**California Housing Prices**](https://github.com/ThinamXx/CaliforniaHousing__Prices.git)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2013.PNG)

**Day14 of 300DaysOfData!**
- **Confusion Matrix**: Confusion Matrix is a better way to evaluate the performance of a Classifier. The general idea of Confusion Matrix is to count the number of times instances of Class A are classified as Class B. This approach requires to have a set of predictions so that they can be compared to the actual targets. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Classification, Training a Binary Classifier using Stochastic Gradient Descent, Measuring Accuracy using Cross Validation, Implementation of CV, Confusion Matrix, Precision and Recall and their Curves and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of SGD Classifier in MNIST Dataset along with Precision and Recall using Python here in the Snapshots. I have also presented the curves of Precision and Recall here. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. I am excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2014a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2014b.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2014c.PNG)

**Day15 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about The ROC Curve, Random Forest Classifier, SGD Classifier, Multi Class Classification, One vs One and One vs All Strategies, Cross Validation, Error Analysis using Confusion Matrix, Multi Class Classification, KNeighbors Classifier, Multi Output Classification, Noises, Precision and Recall Tradeoff and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have completed the Topic Classification from this Book. I have presented the Implementation of The ROC Curve, Random Forest Classifier in Multi Class Classification, The One vs One Strategy, Standard Scaler, Error Analysis, Multi Label Classification and Multi Output Classification using Scikit Learn here in the Snapshots. I hope you will also work on the same. I hope you will also spend some time reading the Topics and Book mentioned above. I am excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2015a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2015b.PNG)

**Day16 of 300DaysOfData!**
- **Ridge Regression**: Ridge Regression is a regularized Linear Regression viz. a regularization term is added to the cost function which forces the learning algorithm to not only fit the Data but also keep the model weights as small as possible. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Training the Models, Linear Regression, The Normal Equations and Computational Complexity, Cost Function and Gradient Descent such as Batch Gradient Descent, Convergence Rate, Stochastic Gradient Descent, Mini batch Gradient Descent, Polynomial Regression and Poly Features, Learning Curves, Bias and Variance Tradeoff, Regularized Linear Models such as Ridge Regression and few more related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Polynomial Regression, Learning Curves and Ridge Regression along with Visualization using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2016.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2016b.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2016a.PNG)

**Day17 of 300DaysOfData!**
- **Elastic Net**: Elastic Net is a middle grouped between Ridge Regression and Lasso Regression. The regularization term **r** is a simple mix of both Ridge and Lasso's regularization terms. When **r** equals 0, it is equivalent to Ridge and when **r** equals 1, it is equivalent to Lasso Regression. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Lasso Regression, Elastic Net, Early Stopping, SGD Regressor, Logistic Regression, Estimating Probabilities, Training and Cost Function, Sigmoid Function, Decision Boundaries, Softmax Regression or Multinomial Logistic Regression, Cross Entropy and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have just started reading the Topic Support Vector Machines. I have presented the simple Implementation of Lasso Regression, Elastic Net, Early Stopping, Logistic Regression and Softmax Regression using Scikit Learn here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2017a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2017.PNG)

**Day18 of 300DaysOfData!**
- **Support Vector Machines**: A Support Vector Machines or SVM is a very powerful and versatile Machine Learning model which is capable of performing Linear and Nonlinear Classification, Regression and even outlier detection. SVMs are particularly well suited for classification of complex but medium sized datasets. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Support Vector Machines, Linear SVM Classification, Soft Margin Classification, Nonlinear SVM Classification, Polynomial Regression, Polynomial Kernel, Adding Similarity Features, Gaussian RBF Kernel, Computational Complexity, SVM Regression which is Linear as well Nonlinear and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Nonlinear SVM Classification using SVC and Linear SVC along with Visualization using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2018a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2018b.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2018c.PNG)

**Day19 of 300DaysOfData!**
- **Voting Classifiers**: Voting Classifiers are the classifiers which aggregates the predictions of different Classifiers and predicts the class that gets the most votes. The majority vote classifier is called a Hard Voting Classifier. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Ensemble Learning and Random Forests, Voting Classifiers such as Hard Voting and Soft Voting Classifiers and few more topics related to the same. Actually, I have also started working on a Research Project with an amazing Team. I have presented the Implementation of Hard Voting and Soft Voting Classifiers using Scikit Learn here in the Snapshots. I hope you will spend some time working on the same and reading the Topics mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2019a.PNG)

**Day20 of 300DaysOfData!**
- **The CART Training Algorithm**: The Algorithm which represents Scikit Learn's Implementation of the Classification and Regression Tree or CART Training algorithm to train Decision Trees also called Growing Trees. It's working principle is splitting the Training set into two subsets using a feature and a threshold. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Decision Functions and Predictions, Decision Trees, Decision Tree Classifier, Making Predictions, Gini Impurity, White Box Models and Black Box Models, Estimating Class Probabilities, The CART Training Algorithm, Computational Complexities, Entropy, Regularization Hyperparameters, Decision Tree Regressor, Cost Function and Instability from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the simple Implementation of Decision Tree Classifier and Decision Tree Regressor along with Visualization of the same using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2020b.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2020a.PNG)

**Day21 of 300DaysOfData!**
- **Bagging and Pasting**: It refers to the approach which uses the same Training Algorithm for every predictor but to train them on different random subsets of the Training set. When sampling is performed with replacement, it is called Bagging and when sampling is performed without replacement, it is called Pasting. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Ensemble Learning and Random Forests, Voting Classifiers, Bagging and Pasting in Scikit Learn, Out of Bag Evaluation, Random Patches and Random Subspaces, Random Forests, Extremely Randomized Trees Ensemble, Feature Importance, Boosting, AdaBoost, Gradient Boosting and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Bagging Ensembles, Decision Trees, Random Forest Classifier, Feature Importance, AdaBoost Classifier and Gradient Boosting using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2021aa.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2021a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2021b.PNG)

**Day22 of 300DaysOfData!**
- **Manifold Learning**: Manifold Learning refers to the Dimensionality Reduction Algorithms that work by modeling the manifold on which the training instances lie which relies on manifold hypothesis which holds that most real world high dimensional datasets to a much lower dimensional manifold. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Gradient Boosting, Early Stopping, Stochastic Gradient Boosting, Extreme Gradient Boosting or XGBoost, Stacking and Blending, Dimensionality Reduction, Curse of Dimensionality, Approaches for Dimensionality Reduction, Projection and Manifold Learning and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Gradient Boosting with Early Stopping along with Visualization using Scikit Learn here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2022a.PNG)

**Day23 of 300DaysOfData!**
- **Incremental PCA**: Incremental PCA or IPCA Algorithms are the algorithms in which we can split the Training set into mini batches and feed an IPCA Algorithm one mini batch at a time. It is useful for large Training sets and also to apply PCA online. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Principal Component Analysis or PCA, Preserving the Variance, Principal Components, Projecting Down the Dimensions, Explained Variance Ratio, Choosing the Right Number of Dimensions, PCA for Compression and Decompression, Reconstruction Error, Randomized PCA, SVD, Incremental PCA and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of PCA, Randomized PCA and Incremental PCA along with Visualizations using Scikit Learn here in the Snapshots. I hope you will spend some time working on the same. I hope you will also spend some time reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2023b.PNG)

**Day24 of 300DaysOfData!**
- **Clustering**: Clustering Algorithms are the algorithms whose goal is to group similar instances together into Clusters. It is a great tool for Data Analysis, Customer Segmentation, Recommender Systems, Search Engines, Image Segmentation, Dimensionality Reduction and many more. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Kernel Principal Component Analysis, Selecting a Kernel and Tuning Hyperparameters, Pipeline and Grid Search, Locally Linear Embedding, Dimensionality Reduction Techniques such as Multi Dimensional Scaling, Isomap and Linear Discriminant Analysis, Unsupervised Learning such as Clustering and KMeans Clustering Algorithm and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Kernel PCA and Grid Search CV, and KMeans Clustering Algorithm along with a Visualization using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2024a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2024b.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2024c.PNG)

**Day25 of 300DaysOfData!**
- **Image Segmentation**: Image Segmentation is the task of partitioning an Image into multiple segments. In Semantic Segmentation, all the pixels that are part of the same object type get assigned to the same segment. In Instance Segmentation, all pixels that are part of the individual object are assigned to the same segment. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about KMeans Algorithms, Centroid Initialization, Accelerated KMeans and Mini Batch KMeans, Finding the Optimal Numbers of Clusters, Elbow rule and Silhouette Coefficient score, Limitations of KMeans, Using Clustering for Image Segmentation and Preprocessing such as Dimensionality Reduction and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Clustering Algorithms for Image Segmentation and Preprocessing along with Visualizations using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2025a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2025c.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2025b.PNG)

**Day26 of 300DaysOfData!**
- **Gaussian Mixtures Model**: A Gaussian Mixture Model or GMM is a probabilistic Model that assumes that the instances were generated from the mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian Distributions form a cluster that typically looks like an Ellipsoid. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about using Clustering Algorithms for Semi Supervised Learning, Active Learning and Uncertainty Sampling, DBSCAN, Agglomerative Clustering, Birch Algorithms, Mean Shift and Affinity Propagation Algorithms, Spectral Clustering, Gaussian Mixtures Model, Expectation Maximization Algorithm and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Clustering Algorithms for Semi supervised Learning and DBSCAN along with Visualizations using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2026a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2026b.PNG)

**Day27 of 300DaysOfData!**
- **Anomaly Detection**: Anomaly Detection also called Outlier Detection is the task of detecting instances that deviate strongly from the norm. These instances are called anomalies or outliers while the normal instances are called inliers. It is useful in Fraud Detection and more. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Gaussian Mixture Models, Anomaly Detection using Gaussian Mixtures, Novelty Detection, Selecting the Number of Clusters, Bayesian Information Criterion, Akaike Information Criterion, Likelihood Function, Bayesian Gaussian Mixture Models, Fast MCD, Isolation Forest, Local Outlier Factor, One Class SVM and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have just started Neural Networks and Deep Learning from this Book. I have presented the Implementation of Gaussian Mixture Model along with Visualizations using Python here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2027a.PNG)

**Day28 of 300DaysOfData!**
- **Rectified Linear Unit Function or ReLU** : It is a continuous but not differentiable at 0 where the slope changes abruptly and makes the Gradient Descent bounce around. It works very well and has the advantage of fast to compute. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Introduction to Artificial Neural Networks with Keras, Biological Neurons, Logical Computations with Neurons, The Perceptron, Hebbian Learning, Multi Layer Perceptron and Backpropagation, Gradient Descent, Hyperbolic Tangent Function and Rectified Linear Unit Function, Regression MLPs, Classification MLPs, Softmax Activation and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Building an Image Classifier using the Sequential API along with Visualization using Keras here in the Snapshots. I hope you will spend some time working on the same and reading the Topics and Book mentioned above. I am excited about the days ahead !!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2028a.PNG)

**Day29 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Creating the Model using Sequential API, Compiling the Model, Loss Function and Activation Function, Training and Evaluating the Model, Learning Curves, Using the Model to make Predictions, Building the Regression MLP using the Sequential API, Building Complex Models using the Functional API, Deep Neural Networks and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Building Regression MLP using Sequential API and Functional API here in the Snapshots. I hope you will gain some insights and you will spend some time working on the same. I hope you will also spend some time reading and Implementing the Topics from the Book mentioned above. I am excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2029a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2029b.PNG)

**Day30 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Building the Complex Models using Functional API, Deep Neural Network Architecture, ReLU Activation Function, Handling Multiple Inputs in the Model, Mean Squared Error Loss Function and Stochastic Gradient Descent Optimizer, Handling Multiple Outputs or Auxiliary Output for Regularization and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Handling Multiple Inputs using Keras Functional API along with the Implementation of Handling Multiple Outputs or Auxiliary Output for Regularization using the same here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. I am excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2030aa.PNG)

**Day31 of 300DaysOfData!**
- **Callbacks and Early Stopping**: Early Stopping is a method that allows you to specify an arbitrarily large number of Training epochs and stopping once the Model stops improving on the validation dataset. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Building Dynamic Models using the Sub Classing API, Sequential API and Functional API, Saving and Restoring the Model, Using Callbacks, Model Checkpoints, Early Stopping, Weights and Biases and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Building Dynamic Models using the Sub Classing API along with the Implementation of Using Callbacks and Early Stopping here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead!!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2031a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2031b.PNG)

**Day32 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Visualization using TensorBoard, Learning Curves, Fine Tuning Neural Network Hyperparameters, Randomized Search CV, Regressor, Libraries to optimize Hyperparameters such as Hyperopt, Talos and few more, Number of Hidden Layers, Number of Neurons per Hidden Layer, Learning Rate, Batch size and Other Hyperparameters and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have also spend some time reading the Paper which is named as **Practical Recommendations for Gradient based Training of Deep Architectures**. Here, I have read about Deep Learning and Greedy Layer Wise Pretraining, Online Learning and Optimization of Generalization Error and few more related to the same. I have presented the Implementation of Tuning Hyperparameters, Keras Regressors and Randomized Search CV here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
- Paper:
  - [**Practical Recommendations for Gradient based Training of Deep Architectures**](https://arxiv.org/pdf/1206.5533.pdf)
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2032a.PNG)

**Day33 of 300DaysOfData!**
- **Vanishing Gradient**: During Backpropagation and calculating Gradients, it often gets smaller and smaller as the Algorithms progresses down to the lower layers which prevents the Training to converge to the good solution. This leads to Vanishing Gradient Problem. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Training Deep Neural Networks, Vanishing and Exploding Gradient Problems, Glorot and He Initialization, Non Saturating Activation Functions, Batch Normalization and its Implementation, Logistic and Sigmoid Activation Function, SELU Activation Function, ReLU Activation Function and Variants, Leaky ReLU and Parametric Leaky ReLU and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Leaky ReLU and Batch Normalization here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2033a.PNG)

**Day34 of 300DaysOfData!**
- **Gradient Clipping**: Gradient Clipping is the Technique to lessen the exploding Gradients problem which simply clip the Gradients during backpropagation so that they never exceed some threshold and it is mostly used in Recurrent Neural Networks. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Gradient Clipping, Batch Normalization, Reusing Pretrained Layers, Deep Neural Networks and Transfer Learning, Unsupervised Pretraining, Restricted Boltzmann Machines, Pretraining on an Auxiliary Task, Self Supervised Learning, Faster Optimizers, Gradient Descent Optimizer, Momentum Optimization, Nesterov Accelerated Gradient and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the simple Implementation of Transfer Learning using Keras and Sequential API here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2034a.PNG)

**Day35 of 300DaysOfData!**
- **Adam Optimization**: Adam which stands for Adaptive Moment Estimation combines the ideas of Momentum Optimization and RMSProp where Momentum Optimization keeps track of an exponentially decaying average of past gradients and RMSProp keeps track of an exponentially decaying average of past squared gradients. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about AdaGrad Algorithm, Gradient Descent, RMSProp Algorithm, Adaptive Moment Estimation or Adam Optimization, Adamax, Nadam Optimization, Training Sparse Models, Dual Averaging, Learning Rate Scheduling, Power Scheduling, Exponential Scheduling, Piecewise Constant Scheduling, Performance Scheduling and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Exponential Scheduling and Piecewise Constant Scheduling here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2035a.PNG) 

**Day36 of 300DaysOfData!**
- **Deep Neural Networks**: The best Deep Neural Networks configurations which will work fine in most cases without requiring much Hyperparameter Tuning is here: Kernel Initializer as LeCun Initialization, Activation Function as SELU, Normalization as None, Regularization as Early Stopping, Optimizer as Nadam, Learning Rate Schedule as Performance Scheduling. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Avoiding Overfitting Through Regularization, L1 and L2 Regularization, Dropout Regularization, Self Normalization, Batch Normalization, Monte Carlo Dropout, Max Norm Regularization, Activation Functions like SELU and Leaky ReLU, Nadam Optimization and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of L2 Regularization and Dropout Regularization using Keras here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2036a.PNG)

**Day37 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Custom Models and Training with TensorFlow, High Level Deep Learning APIs, IO and Preprocessing, Lower Level Deep Learning APIs, Deployment and Optimization, TensorFlow Architecture, Tensors and Operations, Keras Low Level API, Tensors and Numpy, Sparse Tensors, Arrays, String Tensors, Custom Loss Functions, Saving and Loading the Models containing Custom Components and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have also started reading a Book **Speech and Language Processing**. Here, I have read about Regular Expressions, Text Normalization, Tokenization, Lemmatization, Stemming, Sentence Segmentation, Edit Distance and few more Topics related to the same. I have presented the simple Implementation of Custom Loss Function here in the Snapshot. I hope you will also spend some time reading the Topics from the Books mentioned above and below. Excited about the days ahead !!
- Books:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  - [**Speech and Language Processing**](https://web.stanford.edu/~jurafsky/slp3/)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2037a.PNG)

**Day38 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Custom Activation Functions, Initializers, Regularizers and Constraints, Custom Metrics, MAE and MSE, Streaming Metric, Custom Layers, Custom Models, Losses and Metrics based on Models Internals and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have also started reading a Book **Speech and Language Processing**. Here, I have read about Regular Expressions, Basic Regular Expression Patterns, Disjunction, Range, Kleene Star, Wildcard Expression, Grouping and Precedence, Operator Hierarchy, Greedy and Non Greedy matching, Sequence and Anchors, Counters and few more Topics related to the same. I have presented the Implementation of Custom Activation Functions, Initializers, Regularizers, Constraints and Custom Metrics here in the Snapshots. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Books:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  - [**Speech and Language Processing**](https://web.stanford.edu/~jurafsky/slp3/)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2038a.PNG)

**Day39 of 300DaysOfData!**
- **Prefetching and Data API**: Prefetching is the loading of the resource before it is required to decrease the time waiting for that resource. In other words, while the Training Algorithm is working on one batch the dataset will already be working in parallel on getting the next batch ready which will improve the performance dramatically. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Loading and Preprocessing Data using TensorFlow, The Data API, Chaining Transformations, Shuffling the Dataset, Gradient Descent, Interleaving Lines From Multiple Files, Parallelism, Preprocessing the Dataset, Decoding, Prefetching, Multithreading and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the simple Implementation of Data API using TensorFlow here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Books:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2039a.PNG)

**Day40 of 300DaysOfData!**
- **Embedding and Representation Learning**: An Embedding is a trainable dense vector that represents a category. The better the representation of the categories, the easier it will be for the Neural Network to make accurate predictions, so Embeddings must make the useful representations of the categories. This is called Representation Learning. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about The Features API, Column Transformer, Numerical and Categorical Features, Crossed Categorical Features, Encoding Categorical Features using One Hot Vectors and Embeddings, Representation Learning, Word Embeddings, Using Feature Columns for Parsing, Using Feature Columns in the Models and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the simple Implementation of The Features API in Numerical and Categorical Columns along with Parsing here in the Snapshots. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Books:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2040a.PNG)

**Day41 of 300DaysOfData!**
- **Convolutional Layer**:The most important building block of CNN is the Convolutional Layer. Neurons in the first Convolutional Layer are not connected to every single pixel in the Input Image but only to pixels in their respective fields. Similarly, each Neurons in second CL is connected only to neurons located within a small rectangle in the first layer. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Deep Computer Vision using Convolutional Neural Networks, The Architecture of the Visual Cortex, Convolutional Layer, Zero Padding, Filters, Stacking Multiple Feature Maps, Padding, Memory Requirements, Pooling Layer, Invariance, Convolutional Neural Network Architectures and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the simple Implementation of Convolutional Neural Network Architecture here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Books:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2041a.PNG)

**Day42 of 300DaysOfData!**
- **ResNet Model**: Residual Network or ResNet won the ILSVRC 2015 Challenge, developed by Kaiming He, using an extremely deep CNN composed of 152 Layers. This Network uses the Skip connections which is also called Shortcut connections: The signal feeding into a layer is also added to the output of a layer located a bit higher up the stack. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about LeNet-5 Architecture, AlexNet CNN Architecture, Data Augmentation, Local Response Normalization, GoogLeNet Architecture, Inception Module, VGGNet, Residual Network or ResNet, Residual Learning, Xception or Extreme Inception, Squeeze and Excitation Network or SENet and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of ResNet 34 CNN using Keras here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Books:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2042a.PNG)

**Day43 of 300DaysOfData!**
- **Xception Model**: Xception which stands for Extreme Inception is a variant of GoogLeNet Architecture which was proposed in 2016 by Franois Chollet. It merges the ideas of GoogLeNet and ResNet Architecture but it replaces the Inception modules with a special type of layer called a Depthwise Separable Convolution. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Using Pretrained Models from Keras, GoogLeNet and Residual Network or ResNet, ImageNet, Pretrained Models for Transfer Learning, Xception Model, Convolutional Neural Network, Batching, Prefetching, Global Average Pooling and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have presented the Implementation of Pretrained Models such as ResNet and Xception for Transfer Learning here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Books:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2043a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2043b.PNG)

**Day44 of 300DaysOfData!**
- **Semantic Segmentation**: In Semantic Segmentation, each pixel is classified according to the class of the object it belongs to but the different objects of the same class are not distinguished. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Classification and Localization, Crowdsourcing in Computer Vision, Intersection Over Union metric, Object Detection, Fully Convolutional Networks or FCNs, VALID Padding, You Only Look Once or YOLO Architecture, Mean Average Precision or MAP, Convolutional Neural Networks, Semantic Segmentation and few more Topics related to the same from the Book **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**. I have just completed learning from this Book. I have presented the Implementation of Classification and Localization along with the Visualization here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time reading the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Books:
  - **Hands On Machine Learning with Scikit Learn, Keras and TensorFlow**
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2044a.PNG)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2044b.PNG)

**Day45 of 300DaysOfData!**
- **Empirical Risk Minimization**: Training a Model means learning good values for all the weights and the biases from Labeled examples. In Supervised Learning, a Machine Learning Algorithm builds a Model by examining many examples and attempting to find a Model that minimizes loss which is called Empirical Risk Minimization. On my Journey of Machine Learning and Deep Learning, Today I have started learning from the **Machine Learning Crash Course** of Google. Here, I have learned about Machine Learning Philosophy, Fundamentals of Machine Learning and Uses, Labels and Features, Labeled and Unlabeled Example, Models and Inference, Regression and Classification, Linear Regression, Weights and Bias, Training and Loss, Empirical Risk Minimization, Mean Squared Error or MSE, Reducing Loss, Gradient Descent and few more Topics related to the same. I have presented the simple Implementation of Basic Recurrent Neural Network here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned above and below. Excited about the days ahead !!
- Course:
  - [**Machine Learning Crash Course**](https://developers.google.com/machine-learning/crash-course)
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2045.PNG)

**Day46 of 300DaysOfData!**
- On my Journey of Machine Learning and Deep Learning, Today I have learned from **Machine Learning Crash Course** of Google. Here, I have learned and Implemented about Learning Rate or Step size, Hyperparameters in Machine Learning Algorithms, Regression, Gradient Descent, Optimizing Learning Rate, Stochastic Gradient Descent or SGD, Batch and Batch Size, Minibatch Stochastic Gradient Descent, Convergence, Hierarchy of TensorFlow Toolkits and few more Topics related to the same. I have also spend some time in reading the Book **Speech and Language Processing**. Here, I have read about Regular Expressions and Patterns, Precision and Recall, Kleene Star, Aliases for Common Characters, RE Operators for Counting and few more Topics related to the same. I have presented the simple Implementation of Recurrent Neural Network and Deep RNN using Keras here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course and Book mentioned above and below. Excited about the days ahead !!
- Course:
  - [**Machine Learning Crash Course**](https://developers.google.com/machine-learning/crash-course)
- Book:
  - [**Speech and Language Processing**](https://web.stanford.edu/~jurafsky/slp3/)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2046.PNG)

**Day47 of 300DaysOfData!**
- **Feature Vector and Feature Engineering**: Feature Engineering means transforming Raw Data into Feature Vector, which is the set of Floating values comprising the examples of the Dataset. On my Journey of Machine Learning and Deep Learning, Today I have learned from **Machine Learning Crash Course** of Google. Here, I have learned and Implemented about Generalization of Model, Overfitting, Gradient Descent and Loss, Statistical and Computational Learning Theories, Stationarity of Data, Splitting of Data and Validation Set, Representation and Feature Engineering, Feature Vector, Categorical Features and Vocabulary, One Hot Encoding and Sparse Representation, Qualities of Good Features and few more Topics related to the same. I have presented the simple Implementation of RNN along with GRU here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course and Book mentioned above and below. Excited about the days ahead !!
- Course:
  - [**Machine Learning Crash Course**](https://developers.google.com/machine-learning/crash-course)
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2047.PNG)

**Day48 of 300DaysOfData!**
- **Scaling Features**: Scaling means converting floating point Feature Values from their Natural range into Standard range such as 0 to 1. If the Feature set contains multiple Features, then Feature Scaling helps Gradient Descent to converge more quickly. On my Journey of Machine Learning and Deep Learning, Today I have learned from **Machine Learning Crash Course** of Google. Here, I have learned and Implemented about Scaling Feature Values, Handling Extreme Outliers, Binning, Scrubbing the Data, Standard Deviation, Feature Cross and Synthetic Feature, Encoding Nonlinearity, Stochastic Gradient Descent, Cross Product, Crossing One Hot Vectors, Regularization For Simplicity, Generalization Curve, L2 Regularization, Early Stopping, Lambda and Learning Rate and few more Topics related to the same. I have presented the simple Implementation of Linear Regression Model using Sequential API here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned above and below. Excited about the days ahead !!
- Course:
  - [**Machine Learning Crash Course**](https://developers.google.com/machine-learning/crash-course)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2048.PNG)

**Day49 of 300DaysOfData!**
- **Prediction Bias**: Prediction Bias is a quantity that measures how far apart is the average of predictions from the average of labels in Dataset. Prediction Bias is completely a different quantity than Bias. On my Journey of Machine Learning and Deep Learning, Today I have learned from **Machine Learning Crash Course** of Google. Here, I have learned and Implemented about Logistic Regression and Calculating Probability, Sigmoid Function, Binary Classification, Log Loss and Regularization, Early Stopping, L1 and L2 Regularization, Classification and Thresholding, Confusion Matrix, Class Imbalance and Accuracy, Precision and Recall, ROC Curve, Area Under Curve or AUC, Prediction Bias, Calibration Layer, Bucketing, Sparsity, Feature Cross and One Hot Encoding and few more Topics related to the same. I have presented the simple Implementation of Normalization and Binary Classification using Keras here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned above and below. Excited about the days ahead !!
- Course:
  - [**Machine Learning Crash Course**](https://developers.google.com/machine-learning/crash-course)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2049.PNG)

**Day50 of 300DaysOfData!**
- **Categorical Data and Sparse Tensors**: Categorical Data refers to Input Features that represent one or more Discrete items from a finite set of choices. Sparse Tensors are the tensors with very few non zero elements. On my Journey of Machine Learning and Deep Learning, Today I have learned from **Machine Learning Crash Course** of Google. Here, I have learned and Implemented about Neural Networks, Hidden Layers and Activation Functions, Nonlinear Classification and Feature Crosses, Sigmoid Function, Rectified Linear Unit or ReLU, Backpropagation, Vanishing and Exploding Gradients, Dropout Regularization, Multi Class Neural Networks, Softmax, Logistic Regression, Embeddings, Collaborative Filtering, Sparse Features, Principal Component Analysis, Word2Vec and few more Topics related to the same. I have presented the simple Implementation of Deep Neural Networks in Multi Class Classification here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned above and below. Excited about the days ahead !!
- Course:
  - [**Machine Learning Crash Course**](https://developers.google.com/machine-learning/crash-course)
  
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2050.PNG)

**Day51 of 300DaysOfData!**
- **Deep Learning**: Deep Learning is the general class of Algorithms which falls under Artificial Intelligence and deals with training Mathematical entities named Deep Neural Networks by presenting the instructive examples. It uses large amounts of Data to approximate Complex Functions. On my Journey of Machine Learning and Deep Learning, Today I have started reading and Implementing from the Book **Deep Learning with PyTorch**. Here, I have learned about Core PyTorch, Deep Learning Introduction and Revolution, Tensors and Arrays, Deep Learning Competitive Landscape, Utility Libraries, Pretrained Neural Network that recognizes the subject of an Image, ImageNet, Image Recognition, AlexNet and ResNet, Torch Vision Module and few more Topics related to the same from here. I have presented the Implementation of Obtaining Pretrained Neural Networks for Image Recognition using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - [**Deep Learning with PyTorch**](https://www.manning.com/books/deep-learning-with-pytorch)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2051.PNG)

**Day52 of 300DaysOfData!**
- **The GAN Game**: GAN stands for Generative Adversarial Network where Generative means something being created, Adversarial means the two Neural Networks are competing to out smart the other and well Network means Neural Networks. A Cycle GAN can turn Images of one Domain into Images of another Domain without the need for us to explicitly provide matching pairs in the Training set. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book **Deep Learning with PyTorch**. Here, I have learned about Pretrained Models, Generative Adversarial Network or GAN, ResNet Generator and Discriminator Models, Cycle GAN Architecture, Torch Vision Module, Deep Fakes, A Neural Network that turns Horses into Zebras and few more Topics related to the same from here. I have presented the Implementation of Cycle GAN that turns Horses into Zebras using PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - [**Deep Learning with PyTorch**](https://www.manning.com/books/deep-learning-with-pytorch)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2052a.PNG)
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2052b.PNG)
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2052c.PNG)

**Day53 of 300DaysOfData!**
- **Tensors and Multi Dimensional Arrays**: Tensors are the Fundamental Data Structure in PyTorch. A Tensor is an array that is a Data Structure which stores a collection of numbers that are accessible individually using a index and that can be indexed with multiple indices. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book **Deep Learning with PyTorch**. Here, I have learned about A Pretrained Neural Network that describes the scenes, NeuralTalk2 Model, Recurrent Neural Network, Torch Hub, Fundamental Building Block: Tensors, The world as Floating Point Numbers, Multidimensional Arrays and Tensors, Lists and Indexing Tensors, Named Tensors, Einsum, Broadcasting and few more Topics related to the same from here. I have presented the simple Implementation of Indexing Tensors and Named Tensors using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - [**Deep Learning with PyTorch**](https://www.manning.com/books/deep-learning-with-pytorch)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2053.PNG)

**Day54 of 300DaysOfData!**
- **Tensors and Multi Dimensional Arrays**: Tensors are the Fundamental Data Structure in PyTorch. A Tensor is an array that is a Data Structure which stores a collection of numbers that are accessible individually using a index and that can be indexed with multiple indices. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book **Deep Learning with PyTorch**. Here, I have learned about Named Tensors, Changing the names of Named Tensors, Broadcasting Tensors, Unnamed Dimensions, Tensor Element Types, Specifying the Numeric Data Type, The Tensor API, Creation Operations, Indexing, Random Sampling, Serialization, Parallelism, Tensors Storage, Referencing Storage, Indexing into Storage and few more Topics related to the same from here. I have presented the simple Implementation of Named Tensors, Tensor Datatype Attributes and Tensor API using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - [**Deep Learning with PyTorch**](https://www.manning.com/books/deep-learning-with-pytorch)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2054.PNG)

**Day55 of 300DaysOfData!**
- **Encoding Color Channels**: The most common way to encode Colors into numbers is RGB where a color is defined by three numbers representing the Intensity of Red, Green and Blue. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book **Deep Learning with PyTorch**. Here, I have learned about Tensors Metadata such as Size, Offset and Stride, Transposing Tensors without Copying, Transposing in Higher Dimensions, Contiguous Tensors, Managing Tensors Device Attribute such as moving to GPU and CPU, Numpy Interoperability, Generalized Tensors, Serializing Tensors, Data Representation using Tensors, Working with Images, Adding Color Channels, Changing the Layout and few more Topics related to the same from here. I have presented the Implementation of Working with Images such as Changing the Layout and Permute method along with Contiguous Tensors using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - [**Deep Learning with PyTorch**](https://www.manning.com/books/deep-learning-with-pytorch)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2055.PNG)

**Day56 of 300DaysOfData!**
- **Continuous, Ordinal and Categorical Values**: Continuous Values are the values which can be counted and measured along with units. Ordinal Values are the continuous values with no fixed relationships between values. Categorical Values are the enumerations of possibilities. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book **Deep Learning with PyTorch**. Here, I have learned about Normalizing the Image Data, Working with 3D Images or Volumetric Image Data, Representing the Tabular Data, Loading the Data Tensors using Numpy, Continuous Values, Ordinal Values, Categorical Values, Ratio Scale and Interval Scale, Nominal Scale, One Hot Encoding and Embeddings, Singleton Dimensions and few more Topics related to the same from here. I have presented the Implementation of Normalizing the Image Data, Volumetric Data, Tabular Data and One Hot Encoding using PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - [**Deep Learning with PyTorch**](https://www.manning.com/books/deep-learning-with-pytorch)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2056.PNG)

**Day57 of 300DaysOfData!**
- **Continuous, Ordinal and Categorical Values**: Continuous Values are the values which can be counted and measured along with units. Ordinal Values are the continuous values with no fixed relationships between values. Categorical Values are the enumerations of possibilities. On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book **Deep Learning with PyTorch**. Here, I have learned about Continuous and Categorical Data, PyTorch Tensor API, Finding Thresholds in Tabular Data, Advanced Indexing, Working with Time Series Data, Adding Time Dimension in Data, Shaping the Data by Time Period, Tensors and Arrays and few more Topics related to the same from here. I have presented the Implementation of Working with Categorical Data, Time Series Data and Finding Thresholds using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !!
- Book:
  - [**Deep Learning with PyTorch**](https://www.manning.com/books/deep-learning-with-pytorch)

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%2057.PNG)

